{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE ## pip install -U imbalanced-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 6. curso Hello world con TF.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1. Conceptos:\n",
    "    - Investigue las metricas mas usadas para un problema de clasificacion binaria.\n",
    "(puede guiarse de este foro: https://neptune.ai/blog/evaluation-metrics-binary-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2. Problema de clasificacion con imbalanced data\n",
    "      -Crear un modelo de Deep-Learning para la clasificacion de transacciones fraudulentas\n",
    "      -Adicionar un bias inicial para reducir el problema de imbalanced data\n",
    "      -Adicionar un peso en las clases para reducir el problema de imbalanced data\n",
    "      - Usar oversampling y Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Algunas funciones utiles:\n",
    "\n",
    "def plot_cm(tf_data,model, p=0.5):\n",
    "  true_list=[]\n",
    "  pred_list=[]\n",
    "  for ds,lb in tf_data.take(1000):\n",
    "    pred = model.predict(ds)\n",
    "    pred_list.append(pred)\n",
    "    true_list.append(lb)\n",
    "  predictions=np.squeeze(np.concatenate(pred_list))\n",
    "  labels=np.squeeze(np.concatenate(true_list))\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "def plot_roc(tf_data,model):\n",
    "  true_list=[]\n",
    "  pred_list=[]\n",
    "  for ds,lb in tf_data.take(1000):\n",
    "    pred = model.predict(ds)\n",
    "    pred_list.append(pred)\n",
    "    true_list.append(lb)\n",
    "  predictions=np.squeeze(np.concatenate(pred_list))\n",
    "  labels=np.squeeze(np.concatenate(true_list))  \n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, linewidth=2)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,20])\n",
    "  plt.ylim([80,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "def evaluation(tf_data,model):\n",
    "    baseline_results = model.evaluate(tf_data,\n",
    "                                   verbose=1)\n",
    "    for name, value in zip(model.metrics_names, baseline_results):\n",
    "      print(name, ': ', value)\n",
    "    print()\n",
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend();   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion del conjunto de datos: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = tf.keras.utils\n",
    "raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Visualizacion del problema!! Mas clases 0 (no fraude) que 1 (fraude)\n",
    "hist = raw_df['Class'].hist(bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(raw_df['Class'])\n",
    "total = neg + pos\n",
    "print('Muestras:\\n    Total: {}\\n    Positivos: {} ({:.2f}% del total)\\n    Negativos: {} ({:.2f}% del total)\\n'.format(\n",
    "    total, pos, 100 * pos / total,neg, 100 * neg / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "## remover la columna Tiempo (no vamos a realizar series de tiempo!!)\n",
    "\n",
    "cleaned_df.#Complete codigo\n",
    "\n",
    "## Transformar la columna amount en escala logaritmica para que los valores sean mas pequenos!\n",
    "eps = 0.001\n",
    "cleaned_df['Log Ammount'] = np.log(cleaned_df.pop('Amount')+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split train-test!\n",
    "train_data=cleaned_df.sample(frac=0.7,random_state=123)\n",
    "test_data=cleaned_df.drop(train_data.index)\n",
    "validation_data=test_data.sample(frac=0.5,random_state=123)\n",
    "test_data=test_data.drop(validation_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalizar respecto a los valores de training!\n",
    "train_data_X=train_data.copy().pop('Class')\n",
    "normalization = tf.keras.layers.Normalization(axis=-1)\n",
    "normalization.adapt(train_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, repeat =False,batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('Class')\n",
    "  ds = #completar codigo. Hint:use-> from_tensor_slice(x,y)\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  if repeat:\n",
    "    ds= ds.repeat()\n",
    "  ds = ds.batch(#completar codigo.)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_data=df_to_dataset(train_data)\n",
    "val_tf_data=df_to_dataset(validation_data,shuffle=False)\n",
    "test_tf_data=df_to_dataset(test_data,shuffle=False)\n",
    "for ds,lb in train_tf_data.take(1):\n",
    "        print(ds.shape)\n",
    "shaped =ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metricas usadas!!\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo sin considerar imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear un modelo de redes neuronales con la siguente estructura:\n",
    "    -capa de entrada con shape=shaped[-1]\n",
    "    -Incluir capa: normalization\n",
    "    -capa densa de 16 neu. con relu\n",
    "    -capa Dropout con rate=0.2\n",
    "    -capa densa de 16 neu. con relu\n",
    "    -capa Dropout con rate=0.2\n",
    "    -capa densa de 8 neu. con relu\n",
    "    -capa Dropout con rate=0.1\n",
    "    -capa densa de salida (clasificacion binaria) con sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_bare = tf.keras.Sequential([\n",
    "      ##complete codigo\n",
    "      keras.layers.Dense(1, activation='sigmoid'),\n",
    "  ])\n",
    "\n",
    "model_bare.compile(\n",
    "      optimizer= ## use Adam opt con learning_rate=1e-3,\n",
    "      loss= ##complete codigo,\n",
    "      metrics=METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bare = model_bare.fit(\n",
    "    ## datos de entrenamiento,\n",
    "    ##8 epocas,\n",
    "    ## datos de validacion,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_tf_data,model_bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(test_tf_data,model_bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(test_tf_data,model_bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history_bare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo usando bias_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La correccion por bias ($b_0$) viene dado por:\n",
    "\n",
    "$$ p_0 = pos/(pos + neg) = 1/(1+e^{-b_0}) $$\n",
    "$$ b_0 = -log_e(1/p_0 - 1) $$\n",
    "$$ b_0 = log_e(pos/neg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = ##Completar codigo\n",
    "initial_bias =tf.keras.initializers.Constant(initial_bias)\n",
    "initial_bias.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear un modelo de redes neuronales con la siguente estructura:\n",
    "    -capa de entrada con shape=shaped[-1]\n",
    "    -Incluir capa: normalization\n",
    "    -capa densa de 16 neu. con relu\n",
    "    -capa Dropout con rate=0.2\n",
    "    -capa densa de 16 neu. con relu\n",
    "    -capa Dropout con rate=0.2\n",
    "    -capa densa de 8 neu. con relu\n",
    "    -capa Dropout con rate=0.1\n",
    "    -capa densa de salida (clasificacion binaria) con sigmoid y correccion de bias!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias = tf.keras.Sequential([\n",
    "      ##complete codigo\n",
    "      keras.layers.Dense(1, activation='sigmoid',bias_initializer=initial_bias),\n",
    "  ])\n",
    "\n",
    "model_bias.compile(\n",
    "      optimizer=## use Adam opt con learning_rate=1e-3,\n",
    "      loss=##complete codigo\n",
    "      metrics=METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bias = model_bias.fit(\n",
    "    #datos entrenamiento\n",
    "    #epocas 8\n",
    "    #datos validacion \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_tf_data,model_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(test_tf_data,model_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(test_tf_data,model_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar pesos en las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Peso para clase 0: {:.2f}'.format(weight_for_0))\n",
    "print('Peso para clase 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight  = tf.keras.models.clone_model(model_bare)\n",
    "model_weight.compile(\n",
    "      optimizer=## use Adam opt con learning_rate=1e-3,\n",
    "      loss= ##complete codigo\n",
    "      metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_weight = model_weight.fit(\n",
    "    #datos entrenamiento\n",
    "    # 8 epocas\n",
    "    #datos de validacion \n",
    "    verbose=1,\n",
    "    class_weight=class_weight) ##->Pesa las clases!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_tf_data,model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(test_tf_data,model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(test_tf_data,model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar sobre oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dividir los datos entre pos y neg:\n",
    "train_data_neg= train_data[train_data['Class']==0]\n",
    "train_data_pos= train_data[train_data['Class']==1]\n",
    "validation_data_neg= validation_data[validation_data['Class']==0]\n",
    "validation_data_pos= validation_data[validation_data['Class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_data_neg=df_to_dataset(train_data_neg,repeat=True)\n",
    "train_tf_data_pos=df_to_dataset(train_data_pos,repeat=True)\n",
    "val_tf_data_neg=df_to_dataset(validation_data_neg,shuffle=False)\n",
    "val_tf_data_pos=df_to_dataset(validation_data_pos,shuffle=False)\n",
    "for ds,lb in train_tf_data_neg.take(1):\n",
    "        print(ds.shape)\n",
    "shaped =ds.shape\n",
    "resampled_steps_per_epoch = np.ceil(2.0*len(train_data_neg)/shaped[0])\n",
    "resampled_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas:\n",
    "    -Que implica los pesos (weights) en tf.data.Dataset.sample_from_datasets?\n",
    "    -Cambiel los valores  de weights a: [0.9, 0.1],[0.1, 0.9], [0.3, 0.7], [0.7, 0.3] y analize sus resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights =[0.5, 0.5] ## 50% pos, 50%neg\n",
    "resampled_train = tf.data.Dataset.sample_from_datasets([train_tf_data_pos, train_tf_data_neg], weights=weights)\n",
    "resampled_val = tf.data.Dataset.sample_from_datasets([val_tf_data_pos, val_tf_data_neg], weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resampling = tf.keras.models.clone_model(model_bare)\n",
    "\n",
    "\n",
    "model_resampling.compile(\n",
    "      optimizer=## use Adam opt con learning_rate=1e-3,\n",
    "      loss=##complete codigo\n",
    "      metrics=METRICS)\n",
    "\n",
    "history_resample = model_resampling.fit(\n",
    "    resampled_train, ##datos de entrenamiento remuestrado\n",
    "    epochs=18,\n",
    "    validation_data=resampled_val, ##datos de validacion remuestrado\n",
    "    steps_per_epoch=resampled_steps_per_epoch, ##reescalar los pasos por epoca!\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_tf_data,model_resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(test_tf_data,model_resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(test_tf_data,model_resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history_resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=123)\n",
    "def _split_data(df):\n",
    "    X =df.copy()\n",
    "    y = X.pop('Class')\n",
    "    return X,y\n",
    "\n",
    "Xtr, ytr = _split_data(train_data)\n",
    "Xv, yv = _split_data(validation_data)\n",
    "Xts, yts = _split_data(test_data)\n",
    "X_train, y_train = sm.fit_resample(Xtr,ytr)\n",
    "X_val, y_val = sm.fit_resample(Xv,yv)\n",
    "X_test, y_test = sm.fit_resample(Xts,yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ahora contamos con balanced data!!\n",
    "hist = y_train.hist(bins=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3. Realizar el modelo de deep learning para clasificacion usando la base de datos anteriormente balanceada con SMOTE\n",
    "    -Genere el pipeline usando tensor_slices()\n",
    "    -Entrene una red neuronal usando el modelo model_bare usado anteriormente (no use bias_init o weights en    las clases, recuerde que ya tenemos datos balanceados!)\n",
    "    -use en el fit los  Callbacks relacionados con: EarlyStopping, y otro callback de la lista: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks\n",
    "    - Guarde el model completo y subalo al repositorio junto a este notebook completamente terminado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_tf27",
   "language": "python",
   "name": "es_tf27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
